{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Capstone - Udacity\n",
    "\n",
    "## US Immigration Data, Airport, US Demographics and Temprature ETL Pipeline\n",
    "\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "This project augments the US I94 immigration data with further data such as US airport data, US demographics and temperature data to have a wider basis for analysis on the immigration data.\n",
    "\n",
    "The environment created for the project is as follows:\n",
    "- Spark version 3.0 - latest version https://www.apache.org/dyn/closer.lua/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
    "- Hadoop version 3.2 pre-bundled with Spark\n",
    "- Pyspark v3 compatible with Spark version\n",
    "- Library for reading SAS data with https://github.com/saurfang/spark-sas7bdat\n",
    "- Python Anaconda - v1.9.12\n",
    "All the python dependencies were installed for running this project.\n",
    "\n",
    "An ETL pipeline feeds into the 'dwh'(datawarehouse) database within indigeneous Spark Metastore. This database has a fact and dimensions table necessary for studying Immigration behavior. The warehouse can be used as a BI Analytics backend for \n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import io\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope \n",
    "This project augments the US I94 immigration data with further data such as US airport data, US demographics and temperature data to have a wider basis for analysis on the immigration data.\n",
    "An ETL pipeline is built which directly feeds into the Spark warehouse 'dwh'. This warehouse can be used as a BI analytics backend for gaining deep insights on immigration behaviour.\n",
    "\n",
    "#### I94 Immigration Data\n",
    "I94 immigration data comes from theUS National Tourism and Trade Office website. It is provided in SAS7BDAT format which is a binary database storage format.\n",
    "\n",
    "The temperature data is a Kaggle data set that includes temperatures in cities around the world. It can be found here: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "\n",
    "#### World Temperature Data\n",
    "This is defined in the project resources. This dataset came from Kaggle.\n",
    "\n",
    "#### Data for US Demographics\n",
    "This is defined in the project resources. This data comes from OpenSoft.\n",
    "\n",
    "#### Airport Data\n",
    "This is a simple table of airport codes and corresponding cities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data-dict.jpeg                    \u001b[34mi94.csv\u001b[m\u001b[m/\n",
      "\u001b[31mGlobalLandTemperaturesByCity.csv\u001b[m\u001b[m* \u001b[31mi94_apr16_sub.sas7bdat\u001b[m\u001b[m*\n",
      "\u001b[31mairport-codes_csv.csv\u001b[m\u001b[m*            \u001b[34mimmigration\u001b[m\u001b[m/\n",
      "airport.jpeg                      temperature.jpeg\n",
      "demographics.jpeg                 \u001b[31mus-cities-demographics.csv\u001b[m\u001b[m*\n"
     ]
    }
   ],
   "source": [
    "# Data is gathered from the Udacity workspace to local workstation\n",
    "%ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark library to read sas files\n",
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:3.0.0-s_2.12\").\\\n",
    "    enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data source\n",
    "files = [\n",
    "    'GlobalLandTemperaturesByCity.csv',\n",
    "    'i94_apr16_sub.sas7bdat',\n",
    "    'airport-codes_csv.csv',\n",
    "    'us-cities-demographics.csv'\n",
    "]\n",
    "\n",
    "airport_data = 'data/' + files[2]\n",
    "demographics_data = 'data/' + files[3]\n",
    "temperature_data = 'data/' + files[0]\n",
    "immigration_data = 'data/' + files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_immigration = pd.read_sas(immigration_data, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "#df_temperature = pd.read_csv(temperature_data)\n",
    "#df_demographics = pd.read_csv(demographics_data, sep)\n",
    "#df_airports = pd.read_csv(airport_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://deepalis-mbp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x115a4db20>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature Data\n",
    "Converting into Spark Data Frames\n",
    "\n",
    "Saving temperature data in pqt file format while fixing the header and making it consistent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature = spark.read.csv(temperature_data,header=True,inferSchema=True)\n",
    "#fixing the header problem, this will make it consistent\n",
    "for name in list(df_temperature.schema.names):\n",
    "    df_temperature = df_temperature.withColumnRenamed(name, re.sub(r'[^a-zA-Z0-9]', '_', name).lower())\n",
    "    \n",
    "df_temperature.write.mode('overwrite').format('parquet').option('path', 'output/temperature').saveAsTable('temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt| averagetemperature|averagetemperatureuncertainty| city|country|latitude|longitude|\n",
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|              6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01| 5.7879999999999985|           3.6239999999999997|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01|             10.644|           1.2830000000000001|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01| 14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01|             16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-08-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-09-01| 12.780999999999999|                        1.454|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-10-01|               7.95|                         1.63|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-11-01|  4.638999999999999|           1.3019999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-12-01|0.12199999999999987|                        1.756|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-01-01|-1.3330000000000002|                        1.642|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-02-01|             -2.732|                        1.358|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-03-01|              0.129|                        1.088|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-04-01|              4.042|                        1.138|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-05-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-06-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temperature.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration data\n",
    "\n",
    "\n",
    "##### Cleaning steps\n",
    "Fixing data quality and other issues:\n",
    "\n",
    "- date format for dtadfile and dtaddto\n",
    "- data type for i94yr shouldn't be float\n",
    "- data type for i94cit shouldn't be float\n",
    "- data type for i94res shouldn't be float\n",
    "- data type for i94mon shouldn't be float\n",
    "- data type for arrdate shouldn't be float\n",
    "- data type for i94mode shouldn't be float\n",
    "- data type for depdate shouldn't be float\n",
    "- data type for i94bir shouldn't be float\n",
    "- data type for i94visa shouldn't be float\n",
    "- data type for biryear shouldn't be float\n",
    "- data type for count shouldn't be float\n",
    "- data type for cicid shouldn't be float\n",
    "- data type for admnum shouldn't be float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading this as parrquet into the output folder\n",
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(immigration_data)\n",
    "for name in list(df_immigration.schema.names):\n",
    "    df_immigration = df_immigration.withColumnRenamed(name, re.sub(r'[^a-zA-Z0-9]', '_', name).lower())\n",
    "df_immigration.write.mode('overwrite').format('parquet').option('path', 'output/immigration').saveAsTable('immigration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing data quality issues\n",
    "df_imm = (df_immigration\n",
    "                  .withColumn('dtadfile', to_date(df_immigration.dtadfile, 'yyyyMMdd'))\n",
    "                  .withColumn('dtaddto', to_date(df_immigration.dtaddto, 'MMddyyyy'))\n",
    "                  .withColumn('i94yr', df_immigration.i94yr.cast(IntegerType()))\n",
    "                  .withColumn('i94cit', df_immigration.i94cit.cast(IntegerType()))\n",
    "                  .withColumn('i94res', df_immigration.i94res.cast(IntegerType()))\n",
    "                  .withColumn('i94mon', df_immigration.i94mon.cast(IntegerType()))\n",
    "                  .withColumn('arrdate', df_immigration.arrdate.cast(IntegerType()))\n",
    "                  .withColumn('i94mode', df_immigration.i94mode.cast(IntegerType()))\n",
    "                  .withColumn('depdate', df_immigration.depdate.cast(IntegerType()))\n",
    "                  .withColumn('i94bir', df_immigration.i94bir.cast(IntegerType()))\n",
    "                  .withColumn('i94visa', df_immigration.i94visa.cast(IntegerType()))\n",
    "                  .withColumn('biryear', df_immigration.biryear.cast(IntegerType()))\n",
    "                  .withColumn('count', df_immigration['count'].cast(IntegerType()))\n",
    "                  .withColumn('cicid', df_immigration.cicid.cast(LongType()))\n",
    "                  .withColumn('admnum', df_immigration.admnum.cast(LongType()))\n",
    "                  )\n",
    "\n",
    "df_imm.count()\n",
    "\n",
    "#This will be used for staging the data for immigration\n",
    "staging_immigration = df_imm.dropDuplicates(['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+----------+------+------+-------+-----------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|  dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|   dtaddto|gender|insnum|airline|     admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+----------+------+------+-------+-----------+-----+--------+\n",
      "|   29| 2016|     4|   101|   101|    ATL|  20545|      1|     MA|  20561|    62|      2|    1|2016-04-01|     TIA| null|      G|      O|   null|      M|   1954|2016-09-30|     M|  null|     AZ|92503781430|00614|      B2|\n",
      "|  474| 2016|     4|   103|   103|    NEW|  20545|      2|   null|  20547|    25|      2|    1|2016-04-01|    null| null|      G|      O|   null|      M|   1991|2016-06-29|     F|  null|    VES|55410441233|91285|      WT|\n",
      "|  964| 2016|     4|   104|   104|    NEW|  20545|      1|     NY|  20550|    57|      2|    1|2016-04-01|    null| null|      G|      O|   null|      M|   1959|2016-06-29|     F|  null|     UA|55436997033|02067|      WT|\n",
      "| 1677| 2016|     4|   104|   104|    MIA|  20545|      1|     FL|  20576|    50|      2|    1|2016-04-01|    null| null|      G|      O|   null|      M|   1966|2016-06-29|     F|  null|     UX|55460236433|00097|      WT|\n",
      "| 1697| 2016|     4|   104|   104|    MIA|  20545|      1|     FL|  20546|    25|      2|    1|2016-04-01|    null| null|      G|      R|   null|      M|   1991|2016-06-29|     M|  null|     LX|55426537033|00066|      WT|\n",
      "| 1806| 2016|     4|   104|   104|    SFB|  20545|      1|     FL|  20552|    49|      2|    1|2016-04-01|    null| null|      G|      O|   null|      M|   1967|2016-06-29|     F|  null|     OR|55455075633|00317|      WT|\n",
      "| 1950| 2016|     4|   104|   123|    MIA|  20545|      1|     FL|  20561|    57|      2|    1|2016-04-01|    null| null|      G|      O|   null|      M|   1959|2016-06-29|     F|  null|     AA|55457766033|00039|      WT|\n",
      "| 2040| 2016|     4|   104|   104|    MIA|  20545|      1|     FL|  20546|    57|      2|    1|2016-04-01|    null| null|      O|      O|   null|      M|   1959|2016-06-29|  null|  null|     AA|55438157633|00063|      WT|\n",
      "| 2214| 2016|     4|   105|   105|    SLC|  20545|      1|     UT|  20554|    38|      2|    1|2016-04-01|     SOF| null|      G|      O|   null|      M|   1978|2016-09-30|     M|  null|     DL|92481964830|00089|      B2|\n",
      "| 2453| 2016|     4|   107|   107|    CHI|  20545|      1|     IL|  20587|    77|      2|    1|2016-04-01|     KRK| null|      G|      O|   null|      M|   1939|2016-09-30|     F|  null|     AB|92462565330|07420|      B2|\n",
      "| 2529| 2016|     4|   107|   107|    CIN|  20545|      1|     OH|  20553|    30|      1|    1|2016-04-01|     WRW| null|      G|      O|   null|      M|   1986|2016-09-30|     M|  null|     DL|92488583030|00229|      B1|\n",
      "| 2927| 2016|     4|   108|   108|    DET|  20545|      1|     IA|  20555|    73|      2|    1|2016-04-01|    null| null|      G|      O|   null|      M|   1943|2016-06-29|     F|  null|     DL|55449559033|00139|      WT|\n",
      "| 3091| 2016|     4|   108|   108|    NYC|  20545|      1|     MA|   null|     6|      2|    1|2016-04-01|    null| null|      G|   null|   null|   null|   2010|2016-06-29|     M|  null|     DY|55463018533|07011|      WT|\n",
      "| 3506| 2016|     4|   108|   108|    SFR|  20545|      1|     CA|  20555|    54|      2|    1|2016-04-01|    null| null|      G|      O|   null|      M|   1962|2016-06-29|     F|  null|     SK|55447256433|00935|      WT|\n",
      "| 4590| 2016|     4|   111|   111|    AUS|  20545|      1|     TX|  20560|     8|      2|    1|2016-04-01|    null| null|      O|      O|   null|      M|   2008|2016-06-29|  null|  null|     BA|55446379133|00191|      WT|\n",
      "| 4823| 2016|     4|   111|   111|    CLT|  20545|      1|     NY|  20559|    41|      2|    1|2016-04-01|    null| null|      G|      O|   null|      M|   1975|2016-06-29|     F|  null|     AA|55435538733|00787|      WT|\n",
      "| 4894| 2016|     4|   111|   111|    DAL|  20545|      1|     LA|  20552|    67|      2|    1|2016-04-01|    null| null|      G|      O|   null|      M|   1949|2016-06-29|     F|  null|     AA|55437676333|00049|      WT|\n",
      "| 5385| 2016|     4|   111|   111|    NEW|  20545|      1|     NY|  20553|    37|      2|    1|2016-04-01|    null| null|      G|      O|   null|      M|   1979|2016-06-29|     M|  null|     DL|55443051733|00021|      WT|\n",
      "| 5409| 2016|     4|   111|   111|    NEW|  20545|      1|     NY|  20554|    44|      2|    1|2016-04-01|    null| null|      G|      O|   null|      M|   1972|2016-06-29|     M|  null|     DL|55447898033|00149|      WT|\n",
      "| 5556| 2016|     4|   111|   111|    NYC|  20545|      3|     VT|  20547|    28|      2|    1|2016-04-01|    null| null|      Z|      K|   null|      M|   1988|2016-06-29|     M|  null|   null|55459776233| LAND|      WT|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+----------+------+------+-------+-----------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staging_immigration.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographics data\n",
    "\n",
    "##### Cleaning\n",
    "Fixing headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics = spark.read.csv(demographics_data, header=True, sep=';', inferSchema=True)\n",
    "for name in list(df_demographics.schema.names):\n",
    "    df_demographics = df_demographics.withColumnRenamed(name, re.sub(r'[^a-zA-Z0-9]', '_', name).lower())\n",
    "df_demographics.write.mode('overwrite').format('parquet').option('path', 'output/demographics').saveAsTable('demographics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            city|\n",
      "+----------------+\n",
      "|   Silver Spring|\n",
      "|          Quincy|\n",
      "|          Hoover|\n",
      "|Rancho Cucamonga|\n",
      "|          Newark|\n",
      "|          Peoria|\n",
      "|        Avondale|\n",
      "|     West Covina|\n",
      "|        O'Fallon|\n",
      "|      High Point|\n",
      "|          Folsom|\n",
      "|          Folsom|\n",
      "|    Philadelphia|\n",
      "|         Wichita|\n",
      "|         Wichita|\n",
      "|      Fort Myers|\n",
      "|      Pittsburgh|\n",
      "|          Laredo|\n",
      "|        Berkeley|\n",
      "|     Santa Clara|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics.select('city').show()\n",
    "df_demographics.select('city').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|            city|         state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|                race| count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White| 58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...| 24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White| 76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...|  1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...| 11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian| 32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian| 11060|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|American Indian a...|   998|\n",
      "|    Philadelphia|  Pennsylvania|      34.1|         741270|           826172|         1567442|             61995|      205339|                  2.61|        PA|               Asian|122721|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|American Indian a...|  8791|\n",
      "|      Fort Myers|       Florida|      37.3|          36850|            37165|           74015|              4312|       15365|                  2.45|        FL|               White| 50169|\n",
      "|      Pittsburgh|  Pennsylvania|      32.9|         149690|           154695|          304385|             17728|       28187|                  2.13|        PA|               White|208863|\n",
      "|          Laredo|         Texas|      28.8|         124305|           131484|          255789|              4921|       68427|                  3.66|        TX|American Indian a...|  1253|\n",
      "|        Berkeley|    California|      32.5|          60142|            60829|          120971|              3736|       25000|                  2.35|        CA|               Asian| 27089|\n",
      "|     Santa Clara|    California|      35.2|          63278|            62938|          126216|              4426|       52281|                  2.75|        CA|               White| 55847|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demographics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airports data\n",
    "\n",
    "##### Cleaning\n",
    "- fixing headers\n",
    "- fixing the iso_region while removing 'US-' as perfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = spark.read.csv(airport_data, header=True, inferSchema=True)\n",
    "for name in list(df_airport.schema.names):\n",
    "    df_airport = df_airport.withColumnRenamed(name, re.sub(r'[^a-zA-Z0-9]', '_', name).lower())\n",
    "\n",
    "df_airport = df_airport.withColumn('iso_region', regexp_replace('iso_region', r'US-', ''))\n",
    "df_airport.write.mode('overwrite').format('parquet').option('path', 'output/airport').saveAsTable('airport')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|iso_region|\n",
      "+----------+\n",
      "|        PA|\n",
      "|        KS|\n",
      "|        AK|\n",
      "|        AL|\n",
      "|        AR|\n",
      "|        OK|\n",
      "|        AZ|\n",
      "|        CA|\n",
      "|        CA|\n",
      "|        CA|\n",
      "|        CO|\n",
      "|        FL|\n",
      "|        FL|\n",
      "|        FL|\n",
      "|        GA|\n",
      "|        GA|\n",
      "|        HI|\n",
      "|        ID|\n",
      "|        KS|\n",
      "|        IN|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport.select('iso_region').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|        PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|        KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|        AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|        AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|        AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "| 00AS|small_airport|      Fulton Airport|        1100|       NA|         US|        OK|        Alex|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      Cordes Airport|        3810|       NA|         US|        AZ|      Cordes|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|        3038|       NA|         US|        CA|     Barstow|    00CA|     null|      00CA|-116.888000488, 3...|\n",
      "| 00CL|small_airport| Williams Ag Airport|          87|       NA|         US|        CA|       Biggs|    00CL|     null|      00CL|-121.763427, 39.4...|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|        3350|       NA|         US|        CA| Pine Valley|    00CN|     null|      00CN|-116.4597417, 32....|\n",
      "| 00CO|       closed|          Cass Field|        4830|       NA|         US|        CO|  Briggsdale|    null|     null|      null|-104.344002, 40.6...|\n",
      "| 00FA|small_airport| Grass Patch Airport|          53|       NA|         US|        FL|    Bushnell|    00FA|     null|      00FA|-82.2190017700195...|\n",
      "| 00FD|     heliport|  Ringhaver Heliport|          25|       NA|         US|        FL|   Riverview|    00FD|     null|      00FD|-82.3453979492187...|\n",
      "| 00FL|small_airport|   River Oak Airport|          35|       NA|         US|        FL|  Okeechobee|    00FL|     null|      00FL|-80.9692001342773...|\n",
      "| 00GA|small_airport|    Lt World Airport|         700|       NA|         US|        GA|    Lithonia|    00GA|     null|      00GA|-84.0682983398437...|\n",
      "| 00GE|     heliport|    Caffrey Heliport|         957|       NA|         US|        GA|       Hiram|    00GE|     null|      00GE|-84.7339019775390...|\n",
      "| 00HI|     heliport|  Kaupulehu Heliport|          43|       NA|         US|        HI| Kailua-Kona|    00HI|     null|      00HI|-155.980233, 19.8...|\n",
      "| 00ID|small_airport|Delta Shores Airport|        2064|       NA|         US|        ID|  Clark Fork|    00ID|     null|      00ID|-116.213996887207...|\n",
      "| 00IG|small_airport|       Goltl Airport|        3359|       NA|         US|        KS|    McDonald|    00IG|     null|      00IG|-101.395994, 39.7...|\n",
      "| 00II|     heliport|Bailey Generation...|         600|       NA|         US|        IN|  Chesterton|    00II|     null|      00II|-87.122802734375,...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary for Immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/Data-dict.jpeg\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary for Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/temperature.jpeg\" width=400 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary for Airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/airport.jpeg\" width=500 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary for Demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/demographics.jpeg\" width=500 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_immigration.createGlobalTempView(\"immigration\")\n",
    "#df_temperature.createGlobalTempView(\"temperature\")\n",
    "#df_airport.createGlobalTempView(\"airport\")\n",
    "#df_demographics.createGlobalTempView(\"demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----------+\n",
      "|database|   tableName|isTemporary|\n",
      "+--------+------------+-----------+\n",
      "| default|     airport|      false|\n",
      "| default|demographics|      false|\n",
      "| default| immigration|      false|\n",
      "| default| temperature|      false|\n",
      "+--------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "Immigration data has wealth of information on immigration practices in US hence it becomes our fact table.\n",
    "Various dimensions table have been carved out which provides insights on influx of immigrants, visa information for immigrants etc.\n",
    "Demographics and Airports table serve as other dimensions which can be joined with state_code and iata_code\n",
    "\n",
    "\n",
    "#### 3.2 ETL  Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "1. Run 'pythoon create_tables.py' to create tables.\n",
    "2. SQL Join on city to airports data.\n",
    "3. Finally insert the data using the main notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact table:\n",
    "- Immigration (Joined with airports data on iata_code)\n",
    "\n",
    "### Dimension table:\n",
    "- Citywise influx (made from immigration table)\n",
    "\n",
    "### Dimension table:\n",
    "- Demographics table\n",
    "\n",
    "### Dimension table\n",
    "- Visa type (made from immigration table)\n",
    "\n",
    "### Dimension table\n",
    "- Airport (made using airport data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|      dwh|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating a data warehouse\n",
    "\n",
    "spark.sql(\"create database dwh\").show()\n",
    "\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating staging table for immigration\n",
    "staging_immigration.write.mode('overwrite').format('parquet').option('path', 'dwh/s_immigration').saveAsTable('dwh.staging_immigration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Staging data for airport\n",
    "# This is airport\n",
    "\n",
    "# Will be used for loading data to dimension table\n",
    "df_staging_airport = spark.sql(\"select * from airport where iata_code in (select distinct(i94port) from staging_immigration)\")\n",
    "staging_airport = df_staging_airport.dropDuplicates(['iata_code'])\n",
    "staging_airport.write.mode('overwrite').format('parquet').option('path', 'dwh/s_airport').saveAsTable('dwh.staging_airport')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+------------+----------+-----------------+--------+---------+----------+--------------------+\n",
      "| ident|          type|                name|elevation_ft|iso_region|     municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+------+--------------+--------------------+------------+----------+-----------------+--------+---------+----------+--------------------+\n",
      "|  KBGM|medium_airport|Greater Binghamto...|        1636|        NY|       Binghamton|    KBGM|      BGM|       BGM|-75.97979736, 42....|\n",
      "|  KFMY|medium_airport|          Page Field|          17|        FL|       Fort Myers|    KFMY|      FMY|       FMY|-81.8632965087999...|\n",
      "|  KDNS| small_airport|Denison Municipal...|        1274|        IA|          Denison|    KDNS|      DNS|       DNS|-95.38069916, 41....|\n",
      "|  EFHK| large_airport|Helsinki Vantaa A...|         179|     FI-ES|         Helsinki|    EFHK|      HEL|      null|24.963300704956, ...|\n",
      "|  KFOK| small_airport|Francis S Gabresk...|          67|        NY|Westhampton Beach|    KFOK|      FOK|       FOK|-72.6317977905, 4...|\n",
      "|  KHVR|medium_airport|Havre City County...|        2591|        MT|            Havre|    KHVR|      HVR|       HVR|-109.762001, 48.5...|\n",
      "|  KPTK|medium_airport|Oakland County In...|         980|        MI|          Pontiac|    KPTK|      PTK|       PTK|-83.420097351074,...|\n",
      "|  KSNA| large_airport|John Wayne Airpor...|          56|        CA|        Santa Ana|    KSNA|      SNA|       SNA|-117.8679962, 33....|\n",
      "|   CLG|        closed|    Coalinga Airport|        null|        CA|             null|    null|      CLG|      null|-120.360116959, 3...|\n",
      "|  KOPF|medium_airport|Miami-Opa Locka E...|           8|        FL|            Miami|    KOPF|      OPF|       OPF|  -80.278397, 25.907|\n",
      "|  ETAD|medium_airport|Spangdahlem Air Base|        1197|     DE-RP|            Trier|    ETAD|      SPM|      null|6.69250011444, 49...|\n",
      "|  HEBL|medium_airport|  Abu Simbel Airport|         616|    EG-ASN|       Abu Simbel|    HEBL|      ABS|      null|31.611700058, 22....|\n",
      "|  MYNN| large_airport|Lynden Pindling I...|          16|     BS-NP|           Nassau|    MYNN|      NAS|      null|  -77.466202, 25.039|\n",
      "|  KMYR|medium_airport|Myrtle Beach Inte...|          25|        SC|     Myrtle Beach|    KMYR|      MYR|       MYR|-78.9282989502, 3...|\n",
      "|  KPVD| large_airport|Theodore Francis ...|          55|        RI|       Providence|    KPVD|      PVD|       PVD|-71.420403, 41.73...|\n",
      "|  KOAK| large_airport|Metropolitan Oakl...|           9|        CA|          Oakland|    KOAK|      OAK|       OAK|-122.221001, 37.7...|\n",
      "|  KFAR|medium_airport|Hector Internatio...|         902|        ND|            Fargo|    KFAR|      FAR|       FAR|-96.8158035278320...|\n",
      "|AU-CRY| small_airport|Carlton Hill Airport|        null|     AU-WA|     Carlton Hill|    null|      CRY|      null|128.5339965820312...|\n",
      "|   OTT| small_airport| Andre Maggi Airport|         900|     BR-MT|      CotriguaÃ§u|    null|      OTT|      null|-58.581944, -9.89...|\n",
      "|  SSPN| small_airport|  ParanaÃ­ba Airport|        1446|     BR-MS|       ParanaÃ­ba|    SSPN|      PBB|      SSPN|-51.1994018554687...|\n",
      "+------+--------------+--------------------+------------+----------+-----------------+--------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staging_airport.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Staging data for citywise influx\n",
    "staging_influx = spark.sql(\"SELECT i94cit, count(i94cit) count, i94addr, i94port FROM staging_immigration group by i94cit, i94port, i94addr order by count desc\")\n",
    "\n",
    "staging_influx.write.mode('overwrite').format('parquet').option('path', 'dwh/s_influx').saveAsTable('dwh.staging_influx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Staging data for visa info\n",
    "\n",
    "staging_visa = spark.sql(\"SELECT i94port, count(i94port) count, i94visa, visapost, visatype FROM staging_immigration group by visatype, i94visa, i94port, visapost order by count desc\")\n",
    "\n",
    "staging_visa.write.mode('overwrite').format('parquet').option('path', 'dwh/s_visa').saveAsTable('dwh.staging_visa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Staging data for demographics\n",
    "staging_demographics = df_demographics\n",
    "staging_demographics.write.mode('overwrite').format('parquet').option('path', 'dwh/s_demographics').saveAsTable('dwh.staging_demographics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i94port', 'string'),\n",
       " ('count', 'bigint'),\n",
       " ('i94visa', 'int'),\n",
       " ('visapost', 'string'),\n",
       " ('visatype', 'string')]"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staging_visa.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i94cit', 'int'),\n",
       " ('count', 'bigint'),\n",
       " ('i94addr', 'string'),\n",
       " ('i94port', 'string')]"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staging_influx.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ident', 'string'),\n",
       " ('type', 'string'),\n",
       " ('name', 'string'),\n",
       " ('elevation_ft', 'int'),\n",
       " ('iso_region', 'string'),\n",
       " ('municipality', 'string'),\n",
       " ('gps_code', 'string'),\n",
       " ('iata_code', 'string'),\n",
       " ('local_code', 'string'),\n",
       " ('coordinates', 'string')]"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staging_airport.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cicid', 'bigint'),\n",
       " ('i94yr', 'int'),\n",
       " ('i94mon', 'int'),\n",
       " ('i94cit', 'int'),\n",
       " ('i94res', 'int'),\n",
       " ('i94port', 'string'),\n",
       " ('arrdate', 'int'),\n",
       " ('i94mode', 'int'),\n",
       " ('i94addr', 'string'),\n",
       " ('depdate', 'int'),\n",
       " ('i94bir', 'int'),\n",
       " ('i94visa', 'int'),\n",
       " ('count', 'int'),\n",
       " ('dtadfile', 'date'),\n",
       " ('visapost', 'string'),\n",
       " ('occup', 'string'),\n",
       " ('entdepa', 'string'),\n",
       " ('entdepd', 'string'),\n",
       " ('entdepu', 'string'),\n",
       " ('matflag', 'string'),\n",
       " ('biryear', 'int'),\n",
       " ('dtaddto', 'date'),\n",
       " ('gender', 'string'),\n",
       " ('insnum', 'string'),\n",
       " ('airline', 'string'),\n",
       " ('admnum', 'bigint'),\n",
       " ('fltno', 'string'),\n",
       " ('visatype', 'string')]"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staging_immigration.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('city', 'string'),\n",
       " ('state', 'string'),\n",
       " ('median_age', 'double'),\n",
       " ('male_population', 'int'),\n",
       " ('female_population', 'int'),\n",
       " ('total_population', 'int'),\n",
       " ('number_of_veterans', 'int'),\n",
       " ('foreign_born', 'int'),\n",
       " ('average_household_size', 'double'),\n",
       " ('state_code', 'string'),\n",
       " ('race', 'string'),\n",
       " ('count', 'int')]"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staging_demographics.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "|     dwh|     staging_airport|      false|\n",
      "|     dwh|staging_demographics|      false|\n",
      "|     dwh| staging_immigration|      false|\n",
      "|     dwh|      staging_influx|      false|\n",
      "|     dwh|        staging_visa|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use dwh\").show()\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dq_checks(df, staging_table):\n",
    "    \"\"\"Count the number of records for data completeness.\n",
    "    :param df: spark dataframe to check counts on\n",
    "    :param staging_table: corresponding name of table\n",
    "    \"\"\"\n",
    "    total_count = df.count()\n",
    "\n",
    "    if total_count == 0:\n",
    "        print(f\"Data quality check failed for {staging_table} with zero records!\")\n",
    "    else:\n",
    "        print(f\"Data quality check passed for {staging_table} with {total_count} records.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for staging_immigration with 3096313 records.\n",
      "Data quality check passed for staging_visa with 29372 records.\n",
      "Data quality check passed for staging_influx with 89678 records.\n",
      "Data quality check passed for staging_demographics with 2891 records.\n",
      "Data quality check passed for staging_airport with 252 records.\n"
     ]
    }
   ],
   "source": [
    "tables = {\n",
    "    'staging_immigration': staging_immigration,\n",
    "    'staging_visa': staging_visa,\n",
    "    'staging_influx': staging_influx,\n",
    "    'staging_demographics': staging_demographics,\n",
    "    'staging_airport': staging_airport\n",
    "}\n",
    "for table_name, table_df in tables.items():\n",
    "    dq_checks(table_df,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
